{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV8xR-cpppaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6e6953-4926-413d-ddc0-b90f3a807856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-text\n",
        "!pip install simpletransformers\n",
        "!pip install spacy\n",
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install emot\n",
        "!pip install demoji\n",
        "!pip install nlpaug"
      ],
      "metadata": {
        "id": "EfzfL2TE-iRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import contractions\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import regex\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "import demoji\n",
        "import emoji\n",
        "from sklearn.metrics import f1_score\n",
        "demoji.download_codes()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "2Jnez22tGtsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d887be-a2ae-4cf0-da16-49dab0375b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-782662059e0c>:26: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('whitespace')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "ElzFdVZV-NaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Training.tsv',sep='\\t')"
      ],
      "metadata": {
        "id": "2F9Ch-JeQtpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "ZUM4S5LLRy1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'].value_counts()"
      ],
      "metadata": {
        "id": "ImH7VNeuxDnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Preprocessing"
      ],
      "metadata": {
        "id": "UUj7yfYWA4xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#emot_object = emot.core.emot()\n",
        "ps =PorterStemmer()\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "english_stopwords = stopwords.words('english')\n",
        "exclude = set(string.punctuation)\n",
        "def preprocess(text):\n",
        "  #text=demoji.findall(df['Text'])\n",
        "  text = contractions.fix(text.lower(), slang=True)\n",
        "  text= re.sub(r'\\d+', '', text)\n",
        "  text=re.sub(r'$', '', text)\n",
        "  text= re.sub(r'â€™','', text )\n",
        "  text=re.sub('<.*?>','',text)\n",
        "  text=re.sub(r'http\\S+', '', text)\n",
        "  #text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "  text = ''.join(ch for ch in text if ch not in exclude)\n",
        "  tokens = word_tokenize(text)\n",
        "  #print(\"Tokens:\", tokens)\n",
        "  text = [t for t in tokens if t not in english_stopwords]\n",
        "  text = \" \".join(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "Ya7aOTloGV_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "#import demoji\n",
        "#demoji.download_codes()\n",
        "def emo(text):\n",
        "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
        "  temp=temp.replace(\"_\",\"  \")\n",
        "  return temp"
      ],
      "metadata": {
        "id": "xWSmkfcKGZ_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['emo']=data[\"text\"].apply(lambda x:emo(x))\n",
        "data[\"clean_text\"]=data['emo'].apply(lambda X: preprocess(X))\n",
        "#added_data[\"clean_text\"]"
      ],
      "metadata": {
        "id": "VXPJ6pMXGbpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "K_FDByBf9lZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], data['label'], test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "3PKQEPwz9h2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction: TF-IDF (char_wb)"
      ],
      "metadata": {
        "id": "29LIXmksUKlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 5), max_df=1.0, min_df=1, max_features=5000)\n",
        "count_train = Tfidf_vec.fit(X_train)\n",
        "train_features1 = Tfidf_vec.transform(X_train)\n",
        "test_features1 = Tfidf_vec.transform(X_test)"
      ],
      "metadata": {
        "id": "OEfrAmTBXEAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction- (word) TFIDF**"
      ],
      "metadata": {
        "id": "RRBtstDP8RBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tfidf_vec1 = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=5000)\n",
        "# count_train1 = Tfidf_vec1.fit(X_train)\n",
        "# train_features2= Tfidf_vec1.transform(X_train)\n",
        "# test_features2 = Tfidf_vec1.transform(X_test)"
      ],
      "metadata": {
        "id": "O1jNl6hfyuwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Model Building with SVM - LinearSVC"
      ],
      "metadata": {
        "id": "EInql6R1VZt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf1 =LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=10000, random_state=123)\n",
        "clf1.fit(train_features1, y_train)\n",
        "y_pred1=clf1.predict(test_features1)\n",
        "accuracy = accuracy_score(y_test, y_pred1)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred1))"
      ],
      "metadata": {
        "id": "Cyh709M8-gpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf1 =LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=10000, random_state=123)\n",
        "clf1.fit(train_features2, y_train)\n",
        "y_pred2=clf1.predict(test_features2)\n",
        "accuracy = accuracy_score(y_test, y_pred2)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred2))"
      ],
      "metadata": {
        "id": "xUtMcv6U-JXX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}